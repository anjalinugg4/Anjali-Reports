| Date   | Notes
| :----- | :-------------------------------
|7/3 | meet with team for an hour to make changes to the python files, discussed goals for the week/transition to Blender to build Oldenborg, debugged/improved python code with Francisco, continued to work on Blender model, completed SWOT form for one-on-one meeting, followed the WandB Artifact notebook and ran the code
|7/5 | re-read and took notes on the Gibson environment research paper in prep for Thursday's meeting, began the W&B Effective MLOps course and followed along with the code, had my SWOT meeting with Prof Clark, continued the Blender Oldy model
|7/6 | 
|7/7 | 

# Activities

- read into WandB Artifacts
    - tool to track steps of machine learning model/datasets/results
    - tracks changes to the file so that all of us can access these modifications
    - You can store datasets directly in artifact (think of it like a directory of data)
    - when you change the contents in an artifact, W&B will create a new version of the artifact rather than overriding the previous contents
    - W&B creates an alias called "latest" which stores the newest artifact version (you can use indexes to refer to a specific artifact)
- played around with the code on W&B and ran into an error when adding a file to the artifact's contents. This is probably due to the fact that I don't have an actual dataset to add a file to


![Artifact1](/assets/2023-07-10/artifactpt1.png)
![Artifact2](/assets/2023-07-10/artifactpt2.png)
![Artifact3](/assets/2023-07-10/artifactpt3.png)

- continued progress on Blender Oldenborg model:

![Blender model](/assets/2023-07-10/blender.png)

- began the W&B Effective MLOps course and followed along with the code
    - part 1 involved creating an EDA table with the different components of our dataset, initializing artifacts and adding images/labels to it; added this artifact to the table

    ![Artifact Code](/assets/2023-07-10/artifactcode.png)
    ![Create artifact](/assets/2023-07-10/createartifact.png)


    - part 2 involved preparing the data to train our deep learning model; I started a new W&B run with the artifact I created and split the data -> joined this split info with the EDA table I created in part 1

    ![Part 2](/assets/2023-07-10/part2.png)   

    - part 3 involves creating a baseline solution to the semantic segmentation problem (dividing image into predefined classes/categories). However, I ran into issues when I tried to create a dataloader

# Issues
- took me and Francisco a while to figure out why the wait for response function was broken. After trouble shooting, we figured out that self.values was continuously being set to "None" in the while loop, and could never break out of the loop. So, we assigned self.values to a non-None value to fix the function:

Fragment of handle_rotation function:

```
self.values = roll, pitch, yaw
return self.values
```

Now, the wait_for_response function can break out of the while loop:

```
 while not self.values:
        sleep(0.01)
    response = self.values
    self.values = None
    return response
```
- Getting the artifact code debugged while also navigating how it works in relation to our project

- Blender has a pretty steep learning curve, so I'm continuing to explore its different features and figuring out how to build Oldenborg most efficiently 

- After creating my run and artifact in a jupyter notebook, I created a report on wandb but my table of data wasn't loading - I had to refer to the video to get a grasp on what the table was depicting because it wasn't working on my end

- getting an error with creating a dataloaders object when training my data in the W&B Effective MLOps course

![DLS error](/assets/2023-07-10/dataloaderserror.png)

# Plans

- discuss research paper with team
- continue playing around with artifact code and share my findings with the team
- start creating datasets
- finish Oldenborg model
- continue with code review process with Prof Clark and Francisco
- prepare to meet with Oliver and his professor about what we've been working on/how we can best collaborate
- start reading more researh papers/writing summaries to get a grasp on reading and writing academic writing (especially in relation to robotics)

# Article Summaries

- Research Paper Notes for 7/6 Meeting:
    - goal of paper: develop real world perception for active agents
    - virtualize real spaces rather than creating artificial ones
    - robotic aents should have perceptual and physical capabilities
    - newfound idea of extracting complex behaviors involving locomotion/perception of other species and replicating it in technology
    - what is a perceptual active agent? They receive a visual observation from the environment and performs a set of actions that can physically change the said environment
    - learning in the physical world is not ideal because we have to adhere to real-world time/overall costs go up dramatically
    - rendered visual observation in a simulation should be close to what a camera could capture in the real world (photorealism)
    - Gibson: virutal environment for training/testing real-world perceptual agents
    - similar to what we're doing with Unreal Engine: we import a vehicle and place it into a large set of real spaces (subject to physics but can perform any mobility task)
    - goal is to close the perceptual gap in producing a virtual world
    - "Goggles" are a mechanism that makes real images look like renderings (co-domain is makin renderings look more like real images)
    - approach is to implement a combo of rendering a base image and building a nueral network that fills in the dis-occluded areas
    - view in 6D includes (x,y,z) and (roll,pitch,yaw) angles
    - challenge with reconstructing reflective/small objects
    - get clarification on the idea of "point clouds" -> these are all aggregated to make one panorama
    - define bilinear interpolation? My understanding is that it estimates the value between four points using a linear equation 
    - "filler" fixes artifacts and generates a more real looking image
    - strategy in identity initialization is crucial; paper randomly distributed weights but manually altering the weights severely skewed the results and lowered convergence/accuracy 
    - impossible to get rid of the domain gap between the real world and the photo-realistic rendering; forming a "joint space" dissolves this gap
    - agent is randomly placed in environment and gets no direction besides a continuous stream of depth; expected to plan perceptually
    - training is (rendernig,ground truth) but testing is (real image, ground truth) -> explain this distinction more
    - MMD and CORAL values determine how well the domains are aligned
    - limitations of Gibson environment: does not support manipulation, do not have full material properties/no existing physics simulator
    - future test is evaluating Goggles on real robots
