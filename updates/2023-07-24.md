| Date   | Notes
| :----- | :-------------------------------
|7/17 | discussed iGibson paper with team, met with other RAs to go over boxnav code and how to create the conda environment
|7/18 | finished working on generating the screenshot filenames with Francisco, watched the Zoom recording of this morning's meeting about WandB artifacts, prepared for Wednesday's meeting with UCSC colleagues
|7/19 | did presentation and meeting with UCSC colleagues, worked on boxnav code with Francisco and added num_trials feature so that screenshots are not overridden with each trial run, created artifact with larger dataset
|7/20 | started reading new research paper that used igibson as a citation, forked and cloned Oldenborg training from arcslab github, made changes to the inference.py file, created new artifact with 4K images that have no randomized changes in texture, added create_artifact file to Oldenborg Training repo that contains the code required to add an artifact to our arcslab wandb team
|7/21 | 

# Activities

- changed screenshot filename convention to including the degrees truncated to the hundred-thousandth place; moving forward is indicated by 0, rotating right is negative degrees, and rotating left is positive degrees

```
        # Generate the next filename
        angle = str(self.navigator.target_angle).replace(".", "p")
        image_filepath = (
            f"{self.dataset_path}/"
            f"{self.images_saved:06}_{str(action).lower()}_{angle}.{str(self.image_ext).lower()}"
        )
```
```
# Already facing correct direction
        if abs(self.signed_angle_to_target) < self.half_target_wedge:
            action = Action.FORWARD
            self.target_angle = 0

        # Need to rotate left (think of unit circle); rotation indicated by positive degrees
        elif self.signed_angle_to_target > 0:
            action = Action.ROTATE_LEFT
            self.target_angle = f"{-self.signed_angle_to_target:+.2f}"

        # Need to rotate right (think of unit circle); rotation indicated by negative degrees
        else:
            action = Action.ROTATE_RIGHT
            self.target_angle = f"{-self.signed_angle_to_target:+.2f}"
```
![Filenames](/assets/2023-07-24/filenames.png)

- watched the Zoom meeting about creating artifacts and was able to access the data through our WandB team files

![Artifacts](/assets/2023-07-24/teamartifacts.png)

- finished the slideshow presentation with Francisco, recorded a demo video of python OSC and boxnav code to show during tomorrow's meeting

- worked with Francisco to make an artifact with a larger dataset

![Larger Dataset](/assets/2023-07-24/largerdataset.png)

- created even larger dataset (~4,000 images) with no changes in texture

![No texture Dataset](/assets/2023-07-24/notexture.png)

- created create_artifact file for creating artifacts through wandb 

![Artifact Code](/assets/2023-07-24/artifactcode.png)

- used fastai example as a reference for building a training model for the perfect-no_texture artifact

![Show results](/assets/2023-07-24/showresults.png)

# Issues
- tried to figure out how to not override screenshot images when you change the number of runs in the args parser to be anything greater than the default single run; mitigated this by adding a trial_num parameter to the simulate method and wrapper

- then added these lines of code to start at Trial 1 and restart the screenshot saving each time a new trial occurs without overriding previous images

```
    for trial in range(1, args.num_trials + 1):
        simulate(args, trial)
```

- creating an args parser for choosing an artifact's directory location within the create_artifact python file

# Plans
- continue to make training datasets and artifacts, transition into testing
- meet again with Oliver to discuss intersections between our projects
- formulate plan for last week in the lab
- update inference.py file in Oldenborg training repository
- add artifact code to Oldenborg training repo
- train model with fastai code and image+command code
- make additional artifacts (change texture, path, trials/max actions, etc)
- fix wandering path using raycasting




# Article Summaries

- Large-Scale Embodied AI Using Procedural Generation 
    - ProcTHOR samples large, diverse datasets of different envs to train agents
    - navigation, interaction, and manipulation tasks
    - prior models over fit on limited training scenes
    - generates fully interactive envs with physics enabled